ğŸ›  Scrapy í¬ë¡¤ëŸ¬ ì‹¤í–‰
1ï¸âƒ£ Scrapy í¬ë¡¤ëŸ¬ ì‹¤í–‰

sh
ë³µì‚¬
í¸ì§‘
scrapy crawl onion_spider -o onion_data.json
ğŸ‘‰ .onion ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í›„ ê²°ê³¼ë¥¼ onion_data.jsonì— ì €ì¥

2ï¸âƒ£ Scrapy í¬ë¡¤ëŸ¬ ì‹¤í–‰ (ë””ë²„ê¹… ëª¨ë“œ)

sh
ë³µì‚¬
í¸ì§‘
scrapy crawl onion_spider --loglevel=DEBUG
ğŸ‘‰ í¬ë¡¤ë§ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ ë””ë²„ê¹…í•˜ê¸° ìœ„í•´ ë¡œê·¸ë¥¼ ìƒì„¸í•˜ê²Œ ì¶œë ¥

ğŸ“‚ JSON ë°ì´í„° ê°€ê³µ ë° ì €ì¥
3ï¸âƒ£ .onion URLë§Œ ì¶”ì¶œí•˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

sh
ë³µì‚¬
í¸ì§‘
python extract_onion_links.py
ğŸ‘‰ onion_links.jsonì—ì„œ .onion URLë§Œ ì¶”ì¶œí•˜ì—¬ onion_urls.txtë¡œ ì €ì¥

4ï¸âƒ£ extract_onion_links.py ë‚´ìš© ì˜ˆì‹œ:

python
ë³µì‚¬
í¸ì§‘
import json

# JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
with open("onion_links.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# URLë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
onion_urls = [entry["onion_link"].split("redirect_url=")[-1] for entry in data]

# íŒŒì¼ë¡œ ì €ì¥
with open("onion_urls.txt", "w", encoding="utf-8") as f:
    for url in onion_urls:
        f.write(url + "\n")

print("ì¶”ì¶œëœ .onion URLì´ onion_urls.txt íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
ğŸ’¾ CSV íŒŒì¼ì„ JSONìœ¼ë¡œ ë³€í™˜
5ï¸âƒ£ csv_to_json.py ì‹¤í–‰ (CSV â†’ JSON ë³€í™˜)

sh
ë³µì‚¬
í¸ì§‘
python csv_to_json.py
ğŸ‘‰ í¬ë¡¤ë§í•œ .onion ë°ì´í„°ë¥¼ CSVì—ì„œ JSONìœ¼ë¡œ ë³€í™˜

6ï¸âƒ£ csv_to_json.py ë‚´ìš© ì˜ˆì‹œ:

python
ë³µì‚¬
í¸ì§‘
import csv
import json

# CSV íŒŒì¼ì„ JSON íŒŒì¼ë¡œ ë³€í™˜
csv_file = "onion_data.csv"
json_file = "onion_data.json"

# ë°ì´í„° ë³€í™˜
with open(csv_file, "r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    data = [row for row in reader]

# JSON ì €ì¥
with open(json_file, "w", encoding="utf-8") as f:
    json.dump(data, f, indent=4)

print(f"{json_file} íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
ğŸ•µ Scrapy ë¡œê·¸ í™•ì¸ ë° ì¬ì‹œë„ ì„¤ì •
7ï¸âƒ£ Scrapy í¬ë¡¤ëŸ¬ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì • í›„ ì‹¤í–‰

sh
ë³µì‚¬
í¸ì§‘
scrapy crawl onion_spider -s RETRY_ENABLED=True -s RETRY_TIMES=5 -s DOWNLOAD_DELAY=2
ğŸ‘‰ .onion ì‚¬ì´íŠ¸ì—ì„œ ìš”ì²­ì´ ì‹¤íŒ¨í•  ê²½ìš° ìµœëŒ€ 5ë²ˆê¹Œì§€ ì¬ì‹œë„í•˜ë©°, ìš”ì²­ ê°„ 2ì´ˆ ì§€ì—°

ğŸ¯ ì „ì²´ ìë™ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
8ï¸âƒ£ ëª¨ë“  ì‘ì—…ì„ í•œ ë²ˆì— ì‹¤í–‰í•˜ëŠ” run_all.sh (ë¦¬ëˆ…ìŠ¤/macOS)

sh
ë³µì‚¬
í¸ì§‘
#!/bin/bash

echo "Scrapy í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘..."
scrapy crawl onion_spider -o onion_data.json

echo "JSON ë°ì´í„°ì—ì„œ .onion URL ì¶”ì¶œ ì¤‘..."
python extract_onion_links.py

echo "CSV íŒŒì¼ì„ JSONìœ¼ë¡œ ë³€í™˜ ì¤‘..."
python csv_to_json.py

echo "ëª¨ë“  ì‘ì—… ì™„ë£Œ!"
ğŸ‘‰ ì‹¤í–‰ ë°©ë²•:

sh
ë³µì‚¬
í¸ì§‘
chmod +x run_all.sh  # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
./run_all.sh         # ì‹¤í–‰
