import os
import json
import sqlite3
import time
import requests
import schedule
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import re  # ë‚ ì§œ ë³€í™˜ì„ ìœ„í•œ ì •ê·œ í‘œí˜„ì‹

# ğŸ”¥ Tor í”„ë¡ì‹œ ì„¤ì •
PROXIES = {
    "http": "socks5h://127.0.0.1:9050",
    "https": "socks5h://127.0.0.1:9050",
}

# ğŸ“‚ SQLite DB ì„¤ì •
DB_FILE = "victim.db"

# âœ… 1ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
def setup_database():
    """SQLite ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë° í…Œì´ë¸” ì´ˆê¸°í™”"""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS leaks (
            site TEXT NOT NULL,
            domain TEXT NOT NULL UNIQUE,
            upload_time TEXT DEFAULT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)

    conn.commit()
    conn.close()
    print("âœ… ë°ì´í„°ë² ì´ìŠ¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")


# âœ… 2ï¸âƒ£ ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜
def convert_date_format(date_str):
    """DD/MM/YYYY â†’ YYYY-MM-DD ë³€í™˜"""
    match = re.match(r"(\d{2})/(\d{2})/(\d{4})", date_str)
    if match:
        day, month, year = match.groups()
        return f"{year}-{month}-{day}"  # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    return None  # ì˜ëª»ëœ í˜•ì‹ì´ë©´ None ë°˜í™˜

# âœ… 3ï¸âƒ£ Seleniumì„ ì‚¬ìš©í•œ `.onion` ì‚¬ì´íŠ¸ í¬ë¡¤ë§
def crawl_leaks_with_selenium(target_url):
    """Seleniumì„ ì‚¬ìš©í•˜ì—¬ JavaScript ê¸°ë°˜ `.onion` ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""

    options = Options()
    options.headless = True  # ğŸ›‘ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ì„ ìœ„í•´ True ì„¤ì •
    options.set_preference("network.proxy.type", 1)
    options.set_preference("network.proxy.socks", "127.0.0.1")
    options.set_preference("network.proxy.socks_port", 9050)
    options.set_preference("network.proxy.socks_remote_dns", True)  # DNSë¥¼ Tor í†µí•´ ì²˜ë¦¬

    driver = webdriver.Firefox(options=options)

    try:
        print(f"ğŸ” {target_url} ì ‘ì† ì¤‘...")
        driver.get(target_url)

        # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
        WebDriverWait(driver, 90).until(
            EC.presence_of_element_located((By.CLASS_NAME, "cls_recordMiddle"))  # ë„ë©”ì¸ ë°ì´í„°ê°€ ìˆëŠ” ìš”ì†Œ
        )
        time.sleep(10)  # ì¶”ê°€ ëŒ€ê¸° (JS ë™ì  ë°ì´í„° ë¡œë“œ ê³ ë ¤)

        # í˜ì´ì§€ ìŠ¤í¬ë¡¤ì„ ì•„ë˜ë¡œ ë‚´ë ¤ì„œ ì¶”ê°€ ë°ì´í„° ë¡œë”© ìœ ë„
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(5)

        # ğŸ” í¬ë¡¤ë§ëœ HTML ë¶„ì„
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, "html.parser")
        extracted_data = []

        # ğŸ” `.cls_recordMiddle` ë° `.cls_recordBottom` í´ë˜ìŠ¤ë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë“  ìš”ì†Œì—ì„œ ë„ë©”ì¸ + Action date ì¶”ì¶œ
        records = soup.find_all("div", class_="cls_record")
        for record in records:
            # ë„ë©”ì¸ ì •ë³´ ì°¾ê¸°
            domain_element = record.find("div", class_="cls_recordMiddle")
            action_date_element = None

            # `cls_recordBottom` ë‚´ë¶€ì—ì„œ "Action date:"ë¥¼ ì°¾ê³ , ê·¸ ë‹¤ìŒ <div>ì„ ê°€ì ¸ì˜´
            record_bottom = record.find("div", class_="cls_recordBottom")
            if record_bottom:
                date_labels = record_bottom.find_all("div", class_="cls_headerSmall")
                for label in date_labels:
                    if "Action date:" in label.get_text(strip=True):
                        action_date_element = label.find_next_sibling("div")
                        break  # ì²« ë²ˆì§¸ ë§¤ì¹­ ìš”ì†Œë§Œ ì‚¬ìš©

            # ë„ë©”ì¸ê³¼ ë‚ ì§œ ë°ì´í„° ì¶”ì¶œ
            if domain_element and action_date_element:
                domain_text = domain_element.get_text(strip=True)
                action_date_text = action_date_element.get_text(strip=True)
                action_date_text = convert_date_format(action_date_text)  # ë‚ ì§œ ë³€í™˜

                # URL í˜•ì‹ í™•ì¸ ë° ë°ì´í„° ì¶”ê°€
                if domain_text.startswith("http") and "." in domain_text and action_date_text:
                    extracted_data.append((domain_text, action_date_text))

        return extracted_data

    except Exception as e:
        print(f"âŒ Selenium í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return []

    finally:
        driver.quit()

# âœ… 4ï¸âƒ£ ë°ì´í„° ì €ì¥ í•¨ìˆ˜ (upload_time í¬í•¨)
def save_to_db(site, domain, upload_time):
    """DBì— ë°ì´í„° ì €ì¥ ë° ì¤‘ë³µ í™•ì¸"""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    try:
        cursor.execute("""
            INSERT INTO leaks (site, domain, upload_time)
            VALUES (?, ?, ?)
            ON CONFLICT(domain) DO UPDATE SET upload_time = excluded.upload_time
        """, (site, domain, upload_time))
        conn.commit()
        return True  # ìƒˆ ë°ì´í„° ì¶”ê°€ë¨
    except sqlite3.IntegrityError:
        return False  # ì¤‘ë³µ ë°ì´í„°
    finally:
        conn.close()

# âœ… 5ï¸âƒ£ ì‹¤í–‰ ì½”ë“œ (1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰)
def job():
    """í¬ë¡¤ë§ ë° DB ì €ì¥ ì‘ì—… ì‹¤í–‰"""
    setup_database()
    target_url = "http://zohlm7ahjwegcedoz7lrdrti7bvpofymcayotp744qhx6gjmxbuo2yid.onion/"

    print(f"ğŸ” {target_url} í¬ë¡¤ë§ ì¤‘...")
    extracted_data = crawl_leaks_with_selenium(target_url)

    if extracted_data:
        new_count = 0
        for domain, upload_time in extracted_data:
            if save_to_db(target_url, domain, upload_time):
                new_count += 1

        # âœ… ìƒˆë¡œìš´ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì¶œë ¥
        if new_count > 0:
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! {new_count}ê°œì˜ ìƒˆë¡œìš´ ë°ì´í„° ì €ì¥ë¨")
        else:
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ")

# âœ… ìµœì´ˆ ì‹¤í–‰ ì¦‰ì‹œ ìˆ˜í–‰
job()

# ğŸ”¥ 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰í•˜ë„ë¡ ìŠ¤ì¼€ì¤„ ì„¤ì •
schedule.every(1).hours.do(job)

# **ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰**
if __name__ == "__main__":
    print("ğŸ”„ ëœì„¬ì›¨ì–´ ì‚¬ì´íŠ¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (1ì‹œê°„ë§ˆë‹¤ ìë™ ì‹¤í–‰)...")
    while True:
        schedule.run_pending()
        time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ í™•ì¸
