import requests
import sqlite3
import json
from bs4 import BeautifulSoup
import time
import re
from urllib.parse import urlparse, urljoin

new_domain_count = 0

# ì‚¬ìš©ì ì •ì˜ User-Agent í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/103.0.0.0 Safari/537.36"
}

# Tor í”„ë¡ì‹œ ì„¸ì…˜ ìƒì„±
def get_tor_session():
    session = requests.Session()
    session.proxies = {
        "http": "socks5h://127.0.0.1:9050",
        "https": "socks5h://127.0.0.1:9050"
    }
    session.headers.update(HEADERS)
    return session

# SQLite DB ì„¤ì •
DB_FILE = "victim.db"

def init_db():
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS leaks (
            site TEXT NOT NULL,
            domain TEXT NOT NULL UNIQUE,
            upload_time TEXT DEFAULT NULL,
            country TEXT DEFAULT 'Unknown',
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    return conn

# âœ… DBì—ì„œ ë„ë©”ì¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
def is_domain_in_db(conn, domain):
    """DBì— í•´ë‹¹ ë„ë©”ì¸ì´ ìˆëŠ”ì§€ í™•ì¸"""
    cursor = conn.cursor()
    cursor.execute("SELECT 1 FROM leaks WHERE domain = ?", (domain,))
    exists = cursor.fetchone() is not None
    return exists

# WHOIS API ì„¤ì •
WHOISXML_API_KEY = ""

def get_domain_country(domain, max_retries=3, timeout=10):
    """WHOIS APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë„ë©”ì¸ì˜ êµ­ê°€ ì •ë³´ë¥¼ ì¡°íšŒ (ì¬ì‹œë„ ë° ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨)"""
    api_url = f"https://www.whoisxmlapi.com/whoisserver/WhoisService?apiKey={WHOISXML_API_KEY}&domainName={domain}&outputFormat=JSON"

    for attempt in range(1, max_retries + 1):
        try:
            response = requests.get(api_url, timeout=timeout)
            if response.status_code == 200:
                data = response.json()
                country = data.get("WhoisRecord", {}).get("registrant", {}).get("country", "Unknown")
                return country if country else "Unknown"

            print(f"âš ï¸ WHOIS ìš”ì²­ ì‹¤íŒ¨ (ë„ë©”ì¸: {domain}) - ì‘ë‹µ ì½”ë“œ {response.status_code}")
            return "Unknown"

        except requests.exceptions.ReadTimeout:
            print(f"â³ WHOIS ì¡°íšŒ ì‹œê°„ ì´ˆê³¼ (ë„ë©”ì¸: {domain}, ì¬ì‹œë„ {attempt}/{max_retries})")
        except requests.exceptions.RequestException as e:
            print(f"âŒ WHOIS ìš”ì²­ ì˜¤ë¥˜ (ë„ë©”ì¸: {domain}): {e}")
            return "Unknown"

        # ì¬ì‹œë„ ì „ 5ì´ˆ ëŒ€ê¸°
        time.sleep(5)

    print(f"âŒ ìµœì¢… WHOIS ìš”ì²­ ì‹¤íŒ¨ (ë„ë©”ì¸: {domain}) - ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")
    return "Unknown"

# âœ… ë°ì´í„° ì €ì¥ (ê¸°ì¡´ DBì— ì—†ëŠ” ë„ë©”ì¸ë§Œ WHOIS ì¡°íšŒ)
def store_domain(conn, site, domain, upload_date):
    global new_domain_count
    cursor = conn.cursor()

    if is_domain_in_db(conn, domain):
        return False  # ê¸°ì¡´ ë„ë©”ì¸ì´ë¯€ë¡œ ë¬´ì‹œ

    # ìƒˆë¡œìš´ ë„ë©”ì¸ì´ë¯€ë¡œ WHOIS API ì¡°íšŒ
    country = get_domain_country(domain)

    try:
        cursor.execute('INSERT INTO leaks (site, domain, upload_time, country) VALUES (?, ?, ?, ?)', 
                       (site, domain, upload_date, country))
        conn.commit()
        new_domain_count += 1
        return True
    except sqlite3.IntegrityError:
        return False

# âœ… TLD í•„í„°ë§
# âœ… ìŠ¤í¬ë¦¬ë‹í•  TLD ëª©ë¡ (ì›ë˜ ë¦¬ìŠ¤íŠ¸ 100% ìœ ì§€)
VALID_TLDS = (
    '.com', '.net', '.org', '.edu', '.za', '.ma', '.ca', '.vn', 
    '.bank', '.in', '.th', '.uk', '.my', '.tr', '.br', '.sg', '.es', 
    '.au', '.pl', '.cr', '.it', '.il', '.de', '.af', '.ax', '.al', 
    '.dz', '.as', '.ad', '.ao', '.ai', '.aq', '.ag', '.ar', '.am', 
    '.aw', '.at', '.az', '.bs', '.bh', '.bd', '.bb', '.by', '.be', 
    '.bz', '.bj', '.bm', '.bt', '.bo', '.bq', '.ba', '.bw', '.bv', 
    '.io', '.bn', '.bg', '.bf', '.bi', '.cv', '.kh', '.cm', '.ky', 
    '.cf', '.td', '.cl', '.cn', '.cx', '.cc', '.co', '.km', '.cg', 
    '.cd', '.ck', '.ci', '.hr', '.cu', '.cw', '.cy', '.cz', '.dk', 
    '.dj', '.dm', '.do', '.ec', '.eg', '.sv', '.gq', '.er', '.ee', 
    '.sz', '.et', '.fk', '.fo', '.fj', '.fi', '.fr', '.gf', '.pf', 
    '.tf', '.ga', '.gm', '.ge', '.gh', '.gi', '.gr', '.gl', '.gd', 
    '.gp', '.gu', '.gt', '.gg', '.gn', '.gw', '.gy', '.ht', '.hm', 
    '.va', '.hn', '.hk', '.hu', '.is', '.id', '.ir', '.iq', '.ie', 
    '.im', '.jm', '.jp', '.je', '.jo', '.kz', '.ke', '.ki', '.kp', 
    '.kr', '.kw', '.kg', '.la', '.lv', '.lb', '.ls', '.lr', '.ly', 
    '.li', '.lt', '.lu', '.mo', '.mg', '.mw', '.mv', '.ml', '.mt', 
    '.mh', '.mq', '.mr', '.mu', '.yt', '.mx', '.fm', '.md', '.mc', 
    '.mn', '.me', '.ms', '.mz', '.mm', '.na', '.nr', '.np', '.nl', 
    '.nc', '.nz', '.ni', '.ne', '.ng', '.nu', '.nf', '.mp', '.no', 
    '.om', '.pk', '.pw', '.ps', '.pa', '.pg', '.py', '.pe', '.ph', 
    '.pn', '.pt', '.pr', '.qa', '.re', '.ro', '.ru', '.rw', '.bl', 
    '.sh', '.kn', '.lc', '.mf', '.pm', '.vc', '.ws', '.sm', '.st', 
    '.sa', '.sn', '.rs', '.sc', '.sl', '.sx', '.sk', '.si', '.sb', 
    '.so', '.gs', '.ss', '.lk', '.sd', '.sr', '.sj', '.se', '.ch', 
    '.sy', '.tw', '.tj', '.tz', '.tl', '.tg', '.tk', '.to', '.tt', 
    '.tn', '.tm', '.tc', '.tv', '.ug', '.ua', '.ae', '.gb', '.us', 
    '.um', '.uy', '.uz', '.vu', '.ve', '.vg', '.vi', '.wf', '.eh', 
    '.ye', '.zm', '.zw'
)


def is_valid_domain(domain):
    """ë„ë©”ì¸ì´ ë¯¸ë¦¬ ì •ì˜í•œ TLD ëª©ë¡ ì¤‘ í•˜ë‚˜ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸"""
    return any(domain.endswith(tld) for tld in VALID_TLDS)

# âœ… ë‚ ì§œ ì¶”ì¶œ
def extract_upload_date(tag):
    """íƒœê·¸ ë‚´ë¶€ì—ì„œ ë‚ ì§œ íŒ¨í„´(YYYY-MM-DD ë˜ëŠ” DD/MM/YYYY) ê²€ìƒ‰"""
    text = tag.get_text(separator=" ", strip=True)
    match = re.search(r'\b(?:\d{4}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2}[-/]\d{1,2}[-/]\d{4})\b', text)
    return match.group(0) if match else None

# âœ… ë„ë©”ì¸ & ë‚ ì§œ ì¶”ì¶œ
def extract_domains_and_dates(html):
    """HTMLì—ì„œ ë„ë©”ì¸ê³¼ ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
    soup = BeautifulSoup(html, "html.parser")
    domains_dates = {}
    tags_to_check = ['a', 'strong', 'code', 'span', 'pre', 'p', 'div', 'h1', 'h2', 'h3']

    for tag in soup.find_all(tags_to_check):
        text = tag.get_text(strip=True)
        found_domains = re.findall(r'(?:https?://)?([a-z0-9.-]+\.[a-z]{2,})', text, re.IGNORECASE)
        for domain in found_domains:
            if is_valid_domain(domain) and domain not in domains_dates:
                domains_dates[domain] = extract_upload_date(tag)
    return domains_dates

# âœ… ì¬ê·€ í¬ë¡¤ë§
def crawl(url, depth, max_depth, session, conn, visited):
    if depth > max_depth or url in visited:
        return
    visited.add(url)

    parsed_url = urlparse(url)
    site = parsed_url.netloc

    try:
        response = session.get(url, timeout=30)
        if response.status_code != 200:
            print(f"í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ ({url}): ìƒíƒœ ì½”ë“œ {response.status_code}")
            return

        html = response.text
        domains_dates = extract_domains_and_dates(html)

        for domain, upload_date in domains_dates.items():
            store_domain(conn, site, domain, upload_date)

        # ë‚´ë¶€ ë§í¬ ì¬ê·€ í¬ë¡¤ë§
        soup = BeautifulSoup(html, "html.parser")
        for a in soup.find_all("a", href=True):
            link = urljoin(url, a['href'])
            if urlparse(link).netloc == site:
                crawl(link, depth + 1, max_depth, session, conn, visited)

    except Exception as e:
        print(f"ìš”ì²­ ì‹¤íŒ¨ ({url}): {e}")

# âœ… ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰
def main():
    global new_domain_count

    target_onion_urls = [
        "http://pdcizqzjitsgfcgqeyhuee5u6uki6zy5slzioinlhx6xjnsw25irdgqd.onion/",
        "http://5butbkrljkaorg5maepuca25oma7eiwo6a2rlhvkblb4v6mf3ki2ovid.onion/"
    ]
    
    max_depth = 0
    crawl_interval = 3600  # 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰

    session = get_tor_session()
    conn = init_db()

    while True:
        visited = set()
        new_domain_count = 0
        print("ğŸ” í¬ë¡¤ë§ ì‹œì‘...")

        for url in target_onion_urls:
            crawl(url, 0, max_depth, session, conn, visited)

        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! {new_domain_count}ê°œì˜ ìƒˆë¡œìš´ ë„ë©”ì¸ ì¶”ê°€ë¨")
        print(f"{crawl_interval}ì´ˆ í›„ ë‹¤ì‹œ í¬ë¡¤ë§ ì‹œì‘...")
        time.sleep(crawl_interval)

if __name__ == "__main__":
    main()
